# WebMining-RecSys
# DRR: Deep Reinforcement Learning based Recommendation

Deep Reinforcement Learning‚Äìbased Recommendation (DRR) l√† m√¥ h√¨nh g·ª£i √Ω s·ª≠ d·ª•ng **Actor‚ÄìCritic (DDPG)** ƒë·ªÉ t·ªëi ∆∞u h√≥a quy·∫øt ƒë·ªãnh g·ª£i √Ω theo chu·ªói t∆∞∆°ng t√°c ng∆∞·ªùi d√πng, t·∫≠p trung v√†o **ph·∫ßn th∆∞·ªüng d√†i h·∫°n** thay v√¨ d·ª± ƒëo√°n rating ƒë·ªôc l·∫≠p.

## Model Architecture (MDP Formulation)
H·ªá th·ªëng ƒë∆∞·ª£c m√¥ h√¨nh h√≥a d∆∞·ªõi d·∫°ng Markov Decision Process (MDP) $(S, A, P, R, \gamma)$:
*   **State ($S$):** Bi·ªÉu di·ªÖn l·ªãch s·ª≠ t∆∞∆°ng t√°c t√≠ch c·ª±c c·ªßa ng∆∞·ªùi d√πng $H_t = \{i_1, ..., i_n\}$ qua module State Representation,.
*   **Action ($A$):** M·ªôt vector tham s·ªë li√™n t·ª•c $a \in \mathbb{R}^{1 \times k}$. ƒêi·ªÉm x·∫øp h·∫°ng m·ª•c $i$ ƒë∆∞·ª£c t√≠nh b·∫±ng inner product: $score = i \cdot a^\top$,.
*   **Reward ($R$):** Ph·∫£n h·ªìi t·ª´ ng∆∞·ªùi d√πng (rating) ƒë∆∞·ª£c chu·∫©n h√≥a v·ªÅ kho·∫£ng $[-1, 1]$,.
*   **Discount rate ($\gamma$):** H·ªá s·ªë chi·∫øt kh·∫•u cho ph·∫ßn th∆∞·ªüng d√†i h·∫°n (m·∫∑c ƒë·ªãnh 0.9),.

## State Representation Modules
DRR s·ª≠ d·ª•ng 3 c·∫•u tr√∫c n·∫Øm b·∫Øt t∆∞∆°ng t√°c User-Item:
1.  **DRR-p:** S·ª≠ d·ª•ng to√°n t·ª≠ t√≠ch ph·∫ßn t·ª≠ (element-wise product) $w_a i_a \otimes w_b i_b$ ƒë·ªÉ b·∫Øt ph·ª• thu·ªôc c·∫∑p gi·ªØa c√°c m·ª•c,.
2.  **DRR-u:** T√≠ch h·ª£p th√™m vector nh√∫ng c·ªßa ng∆∞·ªùi d√πng ($u$) v√†o c·∫•u tr√∫c DRR-p.
3.  **DRR-ave:** S·ª≠ d·ª•ng l·ªõp **Weighted Average Pooling** ƒë·ªÉ lo·∫°i b·ªè hi·ªáu ·ª©ng v·ªã tr√≠ (position effects) trong chu·ªói t∆∞∆°ng t√°c ng·∫Øn h·∫°n; ƒë·∫ßu ra c√≥ k√≠ch th∆∞·ªõc $3k$,.

## Training Workflow
S·ª≠ d·ª•ng thu·∫≠t to√°n **Deep Deterministic Policy Gradient (DDPG)**:
*   **Actor Network:** C·∫≠p nh·∫≠t theo Policy Gradient: $\nabla_{\theta}J(\pi_{\theta}) \approx \mathbb{E}[ \nabla_a Q_{\omega}(s, a) \nabla_{\theta} \pi_{\theta}(s) ]$,.
*   **Critic Network:** ∆Ø·ªõc t√≠nh gi√° tr·ªã $Q(s, a)$ b·∫±ng c√°ch t·ªëi thi·ªÉu h√≥a loss TD: $L = \frac{1}{N} \sum (y_i - Q_{\omega}(s_i, a_i))^2$,.
*   **Optimization:** S·ª≠ d·ª•ng **Prioritized Experience Replay (PER)** v√† **Target Networks** v·ªõi chi·∫øn l∆∞·ª£c c·∫≠p nh·∫≠t m·ªÅm (soft replace),.
*   **Exploration:** √Åp d·ª•ng k·ªπ thu·∫≠t $\epsilon$-greedy,.

##  Evaluation Metrics
*   **Offline:** Precision@k, NDCG@k th√¥ng qua ph∆∞∆°ng ph√°p re-ranking t·∫≠p ·ª©ng vi√™n,.
*   **Online Simulator:** Total accumulated rewards d·ª±a tr√™n m√¥i tr∆∞·ªùng gi·∫£ l·∫≠p PMF.

## üìÅ Dataset

* **MovieLens 1M**
* Files:

  ```
  users.dat
  movies.dat
  ratings.dat
  ```
---
<p align="center">
  <img src="DRR/src/FINAL_ANALYSIS_OUTPUT/03_Activity_Heatmap.png" width="48%" />
  <img src="DRR\src\FINAL_ANALYSIS_OUTPUT/06_Occupation_Activity.png" width="48%" />
</p>

![image](DRR/src/FINAL_ANALYSIS_OUTPUT/05_Genre_Age_Heatmap.png)

<p align="center">
  <img src="DRR/src/FINAL_ANALYSIS_OUTPUT/07_Trend_Lifespan.png" width="48%" />
  <img src="DRR\src\FINAL_ANALYSIS_OUTPUT/08_Generation_Gap.png" width="48%" />
</p>
## üõ† Environment Setup

### Python Version

```bash
Python >= 3.8
```

### Install Dependencies

```bash
pip install numpy pandas scipy
pip install tensorflow==2.10.0
pip install scikit-learn matplotlib tqdm
pip install wandb
```


## ‚ñ∂Ô∏è Run Training

```bash
python train.py
```

---

## üß™ Run Evaluation

```bash
python eval.py
```

---

## üìà Results Visualization
![image](DRR/src/save_model/trail-2026-01-23-14/images/progress.png)

## üìä Final Evaluation Results (MovieLens 1M)

**Number of evaluated users:** 1208  
**Evaluation type:** DRR

| Metric        | Value    |
|--------------|---------:|
| Precision@5  | 0.4807   |
| Precision@10 | 0.5099   |
| NDCG@5       | 0.4741   |
| NDCG@10      | 0.4964   |

## üìà Demo App
![demo](Demo/demo1.png)
![demo](Demo/demo2.png)
