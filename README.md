# WebMining-RecSys
# DRR: Deep Reinforcement Learning based Recommendation

Deep Reinforcement Learningâ€“based Recommendation (DRR) lÃ  mÃ´ hÃ¬nh gá»£i Ã½ sá»­ dá»¥ng **Actorâ€“Critic (DDPG)** Ä‘á»ƒ tá»‘i Æ°u hÃ³a quyáº¿t Ä‘á»‹nh gá»£i Ã½ theo chuá»—i tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng, táº­p trung vÃ o **pháº§n thÆ°á»Ÿng dÃ i háº¡n** thay vÃ¬ dá»± Ä‘oÃ¡n rating Ä‘á»™c láº­p.

## Model Architecture (MDP Formulation)
Há»‡ thá»‘ng Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng Markov Decision Process (MDP) $(S, A, P, R, \gamma)$:
*   **State ($S$):** Biá»ƒu diá»…n lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c tÃ­ch cá»±c cá»§a ngÆ°á»i dÃ¹ng $H_t = \{i_1, ..., i_n\}$ qua module State Representation,.
*   **Action ($A$):** Má»™t vector tham sá»‘ liÃªn tá»¥c $a \in \mathbb{R}^{1 \times k}$. Äiá»ƒm xáº¿p háº¡ng má»¥c $i$ Ä‘Æ°á»£c tÃ­nh báº±ng inner product: $score = i \cdot a^\top$,.
*   **Reward ($R$):** Pháº£n há»“i tá»« ngÆ°á»i dÃ¹ng (rating) Ä‘Æ°á»£c chuáº©n hÃ³a vá» khoáº£ng $[-1, 1]$,.
*   **Discount rate ($\gamma$):** Há»‡ sá»‘ chiáº¿t kháº¥u cho pháº§n thÆ°á»Ÿng dÃ i háº¡n (máº·c Ä‘á»‹nh 0.9),.

## State Representation Modules
DRR sá»­ dá»¥ng 3 cáº¥u trÃºc náº¯m báº¯t tÆ°Æ¡ng tÃ¡c User-Item:
1.  **DRR-p:** Sá»­ dá»¥ng toÃ¡n tá»­ tÃ­ch pháº§n tá»­ (element-wise product) $w_a i_a \otimes w_b i_b$ Ä‘á»ƒ báº¯t phá»¥ thuá»™c cáº·p giá»¯a cÃ¡c má»¥c,.
2.  **DRR-u:** TÃ­ch há»£p thÃªm vector nhÃºng cá»§a ngÆ°á»i dÃ¹ng ($u$) vÃ o cáº¥u trÃºc DRR-p.
3.  **DRR-ave:** Sá»­ dá»¥ng lá»›p **Weighted Average Pooling** Ä‘á»ƒ loáº¡i bá» hiá»‡u á»©ng vá»‹ trÃ­ (position effects) trong chuá»—i tÆ°Æ¡ng tÃ¡c ngáº¯n háº¡n; Ä‘áº§u ra cÃ³ kÃ­ch thÆ°á»›c $3k$,.

## Training Workflow
Sá»­ dá»¥ng thuáº­t toÃ¡n **Deep Deterministic Policy Gradient (DDPG)**:
*   **Actor Network:** Cáº­p nháº­t theo Policy Gradient: $\nabla_{\theta}J(\pi_{\theta}) \approx \mathbb{E}[ \nabla_a Q_{\omega}(s, a) \nabla_{\theta} \pi_{\theta}(s) ]$,.
*   **Critic Network:** Æ¯á»›c tÃ­nh giÃ¡ trá»‹ $Q(s, a)$ báº±ng cÃ¡ch tá»‘i thiá»ƒu hÃ³a loss TD: $L = \frac{1}{N} \sum (y_i - Q_{\omega}(s_i, a_i))^2$,.
*   **Optimization:** Sá»­ dá»¥ng **Prioritized Experience Replay (PER)** vÃ  **Target Networks** vá»›i chiáº¿n lÆ°á»£c cáº­p nháº­t má»m (soft replace),.
*   **Exploration:** Ãp dá»¥ng ká»¹ thuáº­t $\epsilon$-greedy,.

##  Evaluation Metrics
*   **Offline:** Precision@k, NDCG@k thÃ´ng qua phÆ°Æ¡ng phÃ¡p re-ranking táº­p á»©ng viÃªn,.
*   **Online Simulator:** Total accumulated rewards dá»±a trÃªn mÃ´i trÆ°á»ng giáº£ láº­p PMF.

## ğŸ“ Dataset

* **MovieLens 1M**
* Files:

  ```
  users.dat
  movies.dat
  ratings.dat
  ```

---

## ğŸ›  Environment Setup

### Python Version

```bash
Python >= 3.8
```

### Install Dependencies

```bash
pip install numpy pandas scipy
pip install tensorflow==2.10.0
pip install scikit-learn matplotlib tqdm
pip install wandb
```


## â–¶ï¸ Run Training

```bash
python train.py
```

---

## ğŸ§ª Run Evaluation

```bash
python eval.py
```

---

## ğŸ“ˆ Results Visualization
![image](DRR/src/save_model/trail-2026-01-23-14/images/progress.png)

## ğŸ“Š Final Evaluation Results (MovieLens 1M)

**Number of evaluated users:** 1208  
**Evaluation type:** DRR

| Metric        | Value    |
|--------------|---------:|
| Precision@5  | 0.4807   |
| Precision@10 | 0.5099   |
| NDCG@5       | 0.4741   |
| NDCG@10      | 0.4964   |

## ğŸ“ˆ Demo App
![demo](Demo/demo1.png)
![demo](Demo/demo2.png)
